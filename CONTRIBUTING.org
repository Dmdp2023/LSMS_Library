#+title: Contributing

* Contributing New Data
** Prerequisites

  Things you'll want to have set up in advance include:

  - A =github.com= account.  You'll be much happier if you upload
    your public =ssh-key= to =github.com= for authentication
    purposes.  This is the service that will track changes to source
    code or other documents, other than those which should be
    ephemeral (e.g., log files) or files managed by =dvc= (see below).

  - The =git= binary on your local computer.

  - =dvc=.  See https://dvc.org/doc.  This is used to manage static,
    large files (which aren't a good fit for =git=).  You'll also need
    to install dependencies which allow =dvc= to work with google
    drive and s3 buckets.  Provided you have a reasonably modern =python=
    installation (if you don't we recommend
    https://www.anaconda.com/), a simple =pip install 'dvc[gdrive]'= and =pip install 'dvc[s3]'= may do the trick.  Some shells (e.g., the default zsh on Macs) may require you to escape the square brackets, e.g., =pip install dvc\[s3\]=.

  - =gnupg=, for managing encryption.

  - =emacs=, for working with the =org-mode= files used in this project.


** Setting Up the LSMS Library
  Follow the following steps to use or add to the LSMS Library.

*** Fork or Clone
   Optionally fork (creating your own version), then clone (using
   either your own or =eligon='s version) the repo to your local
   machine, using 
   #+begin_src sh
   git clone git@github.com:ligon/LSMS_Library.git LSMS_Library
   #+end_src
   (Note the capitalization of LSMS_Library, which we'll assume for
   the purposes of this document).  If you fork, then you should
   replace the "eligon" string in the above with your own username.

   This step will set up all the files managed by git in the local
   directory =LSMS_Library= on your machine.

*** DVC Setup
   DVC manages the versioning of more-or-less static datasets.
   To install,
   #+begin_src sh
   pip install dvc[gdrive]
   pip install dvc[s3]
   #+end_src
The private key for accessing the data on google drive is found in the file =gdrive_data_store.json.gpg=.  The =gpg= string indicates that this file is encrypted, in this case using a secret passphrase.

Similarly, the private key for /accessing/ data on the s3 data store is found in =s3_reader_creds.gpg=.  This is encrypted using the same passphrase mentioned above. To learn the secret passphrase you'll need to ask
   =ligon@berkeley.edu=.  Once you have it:    


   *Authentication via Python*   
   
   Use the helper function provided in the LSMS Library:   
   
   #+begin_src python
   import lsms_library as ll
   ll.authenticate()
   #+end_src
   
   You’ll be prompted to enter the secret passphrase (which you can request from =ligon@berkeley.edu=).  
   If successful, this will unlock the credentials needed for DVC and allow you to access the data in the LSMS Library.

   (If this doesn't work, make sure you have =gpg= installed: https://gnupg.org/download/)
* Streaming dvc files
   A =dvc pull= will download dvc files to your local repository.
   But this may not be the best way to proceed!  In particular, =dvc=
   offers an api which permits one to "stream" or cache files, leaving
   your storage local to the working repository free of big data
   files.

   To illustrate,
   #+begin_src python
     import dvc.api
     import pandas as pd

     with dvc.api.open('BigRemoteFile.dta',mode='rb') as dta:
         df = pd.read_stata(dta)
   #+end_src
   This will result in a =pandas.DataFrame= in RAM, but will use no
   additional disk (except that, depending on what's being used as the
   dvc store, the file may actually be stored in =.dvc/cache=; this
   cache can be cleared with =dvc gc=).

** Pulling dvc files
   If you need the actual file instead of a "stream" you can instead
   "pull" the dvc files, using
   #+begin_src sh
   dvc pull
   #+end_src
   and files should be added from the remote dvc data store to your
   working repository. 

* Adding New Data
** Additional S3 Credentials
Write access to the remote s3 repository requires additional credentials; contact =ligon@berkeley.edu= to obtain these.

** Procedure to Add Data
   To add a new LSMS-style survey to the repo, you'll follow the
   following steps.  Here we give the example of adding a 2015--16
   survey from Uganda, obtained from
   https://microdata.worldbank.org/index.php/catalog/3460.  The same
   steps should work for you /mutatis mutandis/:

  1. Create a directory corresponding to the country or area; e.g., 
     #+begin_src sh
     mkdir Uganda
     #+end_src
  2. Create a /sub/-directory indicating the time period for the
     survey; e.g., 
     #+begin_src sh
     mkdir Uganda/2015-16
     #+end_src
  3. Create a =Documentation= sub-directory for each survey; e.g.,
     #+begin_src sh
     mkdir Uganda/2015-16/Documentation
     #+end_src
     In this directory include the following files:
     - SOURCE :: A text file giving both a url (if available) and
       citation information for the dataset.
     - LICENSE :: A text file containing a description of the license
       or other terms under which you've obtained the data.
  4. Add other documentation useful for understanding the data to the
     =Documentation= sub-directory.

  5. Add all the contents of the =Documentation= folder to the =git= repo;
     e.g., 
     #+begin_src sh
     cd ./Uganda/2015-16/Documentation
     git add .
     git commit -m"Add Uganda 2015-16 documentation to repo."
     git push
     #+end_src

  6. Create a =Data= sub-directory for each survey; e.g.,
     #+begin_src sh
     mkdir Uganda/2015-16/Data
     #+end_src

  7. Obtain a copy of the data you're interested in, perhaps as a zip
     file or other archive.  Store this in some temporary place, and
     unzip (or whatever) the files into the relevant Country/Year/Data
     directory, taking care to preserve any useful directory structure
     in the archive.  E.g.,
     #+begin_src sh
     cd Uganda/2015-16 && unzip -j /tmp/UGA_2015_UNPS_v01_M_STATA8.zip
     #+end_src
  8. Add the data you've unarchived to =dvc=, then add the /pointers/
     (i.e., files with a .dvc extension to git).  For the Uganda case we assume that
     all the relevant data comes in the form of =stata= *.dta files,
     since this is what we downloaded from the World Bank.  For example,
     #+begin_src sh
     cd ../Data
     dvc add *.dta
     git commit -m"Add Uganda/2015-16/Data/*.dta files to dvc store."
     git pull && git push
     #+end_src
  9. Push the data files to the dvc store. Make sure you have good
     internet connection!  Then a simple
     #+begin_src sh
     dvc push
     #+end_src
     will copy the data to the remote data store.  NB: If this is the
     first time you've done this for this repository, then you'll
     first need to jump through some simple hoops to authenticate with
     gdrive.
  10. With the files pushed to the dvc store, you won't need them
      locally anymore, so you can do something like
      #+begin_src sh
      cd ../Data && rm *.dta
      #+end_src
      or (if you have a more complex directory structure) perhaps
      #+begin_src sh
      find ../Data -name \*.dta -exec rm \{\} \;
      #+end_src
* Adding New Data Artifacts Using YAML

This guide explains how to organize ~data_scheme.yml~, ~data_info.yml~, and optional Python transformation scripts to ensure proper functionality of the LSMS package. Maintaining correct indentation and formatting in ~data_info.yml~ is essential for the file to be processed correctly.

** 1. Create the _/ Subdirectory and data_scheme.yml

In each *country directory*, create a subdirectory named _:

Inside the _/ folder, create a ~data_scheme.yml~ file to define the data structure and index for each table:

** 2. Create the _/ Subdirectory in Each Wave and Define data_info.yml

In each *wave directory*, create an _ folder:

Then, inside _/, create a ~data_info.yml~ file to describe the data files and mappings.

*** 2.1 Basic Format

*** 2.2 Composite Index or Variables with Functions

If a variable (e.g., i) requires multiple columns from the original dataset, list them together in a list format in YAML using hyphens (-). For example, if variable i maps to multiple source columns, define it as follows:

** 3. Apply Transformation Functions in Python

The `format_id` function from local_tools is automatically applied to all index variables by default.

To apply a different formatting function to a specific index or column variable, you can define your own function and specify it in one of two ways:
- Option 1: Name the function to match the variable name (e.g., i for variable i), and it will be recognized automatically.
- Option 2: Add 'function: function_name' at the end of the mapping list (see bullet 7 for details). This explicitly assigns the function to mappings.

This approach allows flexible customization of formatting logic for individual variables.

*** 3.1 Wave-Specific Column Transformation

Create a Python file in the wave’s _ folder, e.g., ~Mali/2017-18/_/2017-18.py~:

*** 3.2 Country-Wide Column Transformation

To apply a transformation across all waves in a country, define the function in ~Mali/_/mali.py~.

** 4. Value Mapping Dictionaries

Define value mappings in ~data_info.yml~ for categorical variables.

Note: The empty line after the variable name is required to separate it from the mapping dictionary.

** 5. Row-Level DataFrame Transformation

Define a function named after the dataframe in either the wave- or country-level Python file:

** 6. Multi-File DataFrames

*** 6.1 Vertical Merge: Stacking Data Row-Wise (data from multiple files)

*** 6.2 File-Specific Overrides

** 7. File-Specific Function Applications

To apply a function to a specific column mapping, add 'function: function_name' at the end of the variable mapping list (as shown in the example for variable i). This function will be applied only to files that use this particular column mapping. If an override mapping is present, the function will not be applied.

If you want the function to apply across all mappings for column i, refer to bullet point 3, where you can simply define the function name directly after the variable name i.

** 8. Horizontal Merge: Combining Data Column-Wise (adding columns side by side)

** 9. Hard-Coded Case

In some cases where dataframes require more complex logic than simple mappings, create a Python file named after the dataframe in the wave's _ folder (e.g., ~Malawi/2004-05/_/food_acquired.py~). And create a Makefile in the country-level _ directory (e.g. ~Malawi/_/Makefile~). The system will automatically generate the dataframe by running =make=.

Note: The reason for not just simply running the Python file is because some data's Python files have dependencies.




